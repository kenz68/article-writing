    This paper introduces a class of probabilistic counting algorithms with one can
estimate the number of distinct elements in a large collection of data (typically a large file stored on disk) in a single pass using only a small additional storage (typically less then a hundred binary works) and only a few operations per element scanned. The algorithms are based on statistical observations made on bits of hashed values of records. They are by construction totally insensitive to the replicative structure of elements in the file; they can be used in the context of distributed systems without any degradation of perfromances and prove especially useful in the contect of data bases query optimisation.