    This paper introduces a class of probabilistic counting algorithms with one can
estimate the number of distinct elements in a large collection of data (typically a large file stored on disk) in a single pass using only a small additional storage (typically less then a hundred binary works) and only a few operations per element scanned. The algorithms are based on statistical observations made on bits of hashed values of records. They are by construction totally insensitive to the replicative structure of elements in the file; they can be used in the context of distributed systems without any degradation of perfromances and prove especially useful in the contect of data bases query optimisation.

# Introduction
    As data base systems allow the user to specify more and more complex queries,
the need arises for efficient processing methods. A complex query can however
generally be evalueted in a number of different manners, and the overall performance of a data base system depends rather crucially on the selection of appropriate decomposition strategies in each particular case.
    Even a problem as trivial as computing the intersection of two collections of data A and B lends itself to a number of different treatments.
    1. Sort A, search each element of B in A and retain it if it appears in A;
    2. Sort A, sort B, then perform a merge-like operation to determine the intersection;
    3. eliminate duplicates in A and/or B using hashing or hash filters, the perform Algorithm 1 or 2.
    
    Each of these evaluation strategy will have a cost essentially determined by the number of records a,b in A and B, and the number of distinct elements a,b in A and B, and for typical sorting methods, the costs are:
- for strategy 1: O(alogα + blogβ);
- for strategy 2: O(alogα + blogβ + a + b)... .
#### In a number of similar situations, it appears thus that, apart from the sizes of the files on which one eperates (i.e., the number of records), a major determinant of efficiency is the `cardinalities` of the underlying sets, i.e., the number of distinct elements they comprise.

#### The situation gets much more complex when operations like projections, selections, multiple join in combination with various boolean operations appear in queries. As an example, the relational system `system R` has a sophisticated query optimiser. In order to perform its task, that programme keeps several statistics on relations of the data base. The most important ones are the sizes of relations as well as the number of different elements of some key fields [8]. This information is used to determine the selectivity of attributes at any given time in order to decide the choice of keys and the choice of the appropriate algorithms to be employed when computing relational operators. The choices are made in order to minimise a certain cost function that depends on specific CPU and disk access costs as well as sizes and cardinalities of relation or fields. In system R, this information is periodically recomputed and kept in catalogues that are companions to the data base records and indexes.

#### In this paper, we propose efficient algorithms to estimate the  cardinalities of multisets of data as are commonly encountered in data base practice. A trivial method consists in determining card(M) by bulding a list of all elements of M without replicationl this method has the advantage of being exact but is has a cost in number of disk accesses and auviliary storage (at least O(a) or O(alog.a) if sorting is used) that might be higher than the possible gains which one can obtain using that information.
#### The method we propose here is probabilistic in nature since its result depends on the particular hashing function used and on the particular data on which it operates. It uses minimal extra storage in core and provides practically useful estimates on cardinalities of large collections of data. The accuracy is inversely related to the storage; using 64 binary words of typically 32 bits, we want attain a typical accuracy of 10%; using 256 words, the accuracy improve to about 5%. The performances do not degrade as files get large: with 32 bit words, one can safely count cardinalities well over 100 million. The only asumption made í that records can be hashed in a suitably psedo-uniform manner. This does not however appear to be a severe limitation since empirical studies on large industrial files [5] reveal that careful implementations of standard hashing techniques do achieve practically uniformity of hashed values. Furthermore, by design, our algorithms are totally insensitive to the replication structures of files: as opposed to sampling techniques (The simplest sampling algorithm is: take a sample of size $N_0$ of a file of size N. Estimate the cardinality $v_0$ of the sample using any direct algorithm and return $v_0(N/N_0)$ as estimate of the cardinality of the whole file.) the result will be the same whether elements appear a million times or just a few times.
#### From a more theoretical standpoint, these techniques constitute yet another illustration of the gains that may be achieved in manay situations through the use of probabilistic methods. We mention here Morris' approximate counting algorithm [6] which maintains approximate counters with an expected constant relative accuracy using only $$log_2log_2n + O(1)$$
 